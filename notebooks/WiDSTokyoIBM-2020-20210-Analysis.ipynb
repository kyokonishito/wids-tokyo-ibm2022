{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "↑プロジェクトトークンを挿入したら、実行を忘れずに!\n",
    "\n",
    "\n",
    "# WiDSTokyoIBM-2020-20210-Analysis\n",
    "\n",
    "こちらのQiita記事のものです："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このnotebookはIBMCloud上のWatson Studioのnotebookで動くように書かれていますが、他の環境でも動作すると思いますが、一部Watson Studio特有のCodeがあります。<br>\n",
    "**`IBM Cloud Watson Studioのみ`** と書いてある場合はWatson Studio特有のCodeです。<br>\n",
    "逆に他の環境だと必須のCodeはコメントアウトして **`IBM Cloud Watson Studio以外`** と書いてありますので、他の環境はコメントアウト部分を必要に応じて実行してください。\n",
    "\n",
    "\n",
    "### **`IBM Cloud Watson Studioのみ`**\n",
    "Watson Studio上のnotebookからIBM Cloud Object Storage(ICOS)へのFileの読み書き - project-libを使う -の手順が前準備として必要です。\n",
    "\n",
    "まずは1,2を実施お願いします。\n",
    "\n",
    "- 1.[ Projectを開く](https://qiita.com/nishikyon/items/1bed62a2a98b0c970f40#1-project%E3%82%92%E9%96%8B%E3%81%8F)\n",
    "- 2.[ 前準備アクセス・トークンの作成](https://qiita.com/nishikyon/items/1bed62a2a98b0c970f40#2-%E5%89%8D%E6%BA%96%E5%82%99%E3%82%A2%E3%82%AF%E3%82%BB%E3%82%B9%E3%83%88%E3%83%BC%E3%82%AF%E3%83%B3%E3%81%AE%E4%BD%9C%E6%88%90)\n",
    "\n",
    "その後、このnotebookに\n",
    "\n",
    "- [5: プロジェクト・トークンの挿入](https://qiita.com/nishikyon/items/1bed62a2a98b0c970f40#5-%E3%83%97%E3%83%AD%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E3%83%88%E3%83%BC%E3%82%AF%E3%83%B3%E3%81%AE%E6%8C%BF%E5%85%A5)を実施してください。\n",
    "\n",
    "\n",
    "一番上に挿入されたセルの実行は忘れずにお願いします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Watson Libraryのインポート\n",
    "https://cloud.ibm.com/apidocs/natural-language-understanding?code=python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade ibm-watson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 2. データファイルの読み込み\n",
    " \n",
    " DATA列に分析したいデータを入れたエクセルファイルまたはCSVファイルを準備します。<br>\n",
    " DATA列のみ使いますので、他の列は何が入っていてもOKです。\n",
    " \n",
    " サンプル表：\n",
    " \n",
    " |  KEYID | DATA |\n",
    "| :--- | :--- |\n",
    "|  A0001  |  とてもよい感じで感動しました  |\n",
    "|  B0001 |  見にくい資料があったのでフォントを大きくして欲しい  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## `IBM Cloud Watson Studioのみ`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "詳細は[こちら](https://qiita.com/nishikyon/items/912ef69736fcee4c0793)を参照してください\n",
    "\n",
    "1. 右上の01/00アイコンをクリックし、対象ファイルをドラッグ&ドロップして、ファイルをアップロード\n",
    "2. 下のセルをクリック\n",
    "2. アップロードしたファイルの下にある「コードに挿入」メニューをクリックし、`pandas DataFrame`をクリック\n",
    "3. 下のセルに挿入されたコードの最後の二行が`df_data_0`で始まっていない場合は、`df_data_n`(nは数字) を`df_data_0`に置き換える\n",
    "3. 下のセルを実行\n",
    "\n",
    "修正前コードの例:\n",
    "```\n",
    "df_data_1 = pd.read_excel(body.read())\n",
    "df_data_1.head()\n",
    "```\n",
    "\n",
    "\n",
    "修正後コード(必ず`df_data_0`で始まるようにする):\n",
    "```\n",
    "df_data_0 = pd.read_excel(body.read())\n",
    "df_data_0.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　Watson Studio利用の場合のみ実行\n",
    "# pandas DataFrameメニューをクリックする前に、このセルをクリック！！！！！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `IBM Cloud Watson Studio以外`\n",
    " \n",
    " data_pathに読み込ませたいEXCELファイル　または　CSVファイルをパス付きで指定後、実行してください。\n",
    " \n",
    " \n",
    " 例：\n",
    " ```\n",
    "  data_path= \"/Users/nishito/polls_opentext.xlsx\"\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　ファイルパスを指定して読み込み (Local jupyter実行用)　する場合のみ実行\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# data_path=\"<data_pathに読み込ませたいEXCELファイル　または　CSVファイルをパス付きで指定>\"\n",
    "\n",
    "# ext_str=os.path.splitext(data_path)\n",
    "# if ext_str[1].lower() == \".xlsx\":\n",
    "#     df_data_0 = pd.read_excel(data_path)\n",
    "# elif ext_str[1].lower() == \".csv\":\n",
    "#      df_data_0 = pd.read_csv(data_path)\n",
    "# else:\n",
    "#     print(\"Filepath Error!\")\n",
    "# df_data_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. いろいろ前処理\n",
    "\n",
    "各セルの実行をします。\n",
    "何をやっているかはコメントを参考にしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのコピー\n",
    "df_poll=df_data_0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの整形　　refineTextt列に整形したデータを入れる\n",
    "import numpy as np\n",
    "\n",
    "def refine_text(df_data):\n",
    "    \n",
    "    # データの整形\n",
    "    df_data['refineText'] = df_data['DATA']\n",
    "\n",
    "    # NaN削除\n",
    "    df_data['refineText'] = df_data['refineText'].replace(np.nan, '', regex=True)\n",
    "\n",
    "    # 改行削除\n",
    "    df_data['refineText'] = df_data.apply(lambda x: x['refineText'].replace('\\n', ' '), axis=1)\n",
    "    df_data['refineText'] = df_data.apply(lambda x: x['refineText'].strip(), axis=1)\n",
    "    \n",
    "    return\n",
    "\n",
    "refine_text(df_poll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Natural Language Understanding: apikey, urlのセット\n",
    "\n",
    "Natural Language Understandingのapikey, urlを以下のセルでにセットしてください。\n",
    "\n",
    "\n",
    "```py\n",
    "apikey=\"<apileyの文字列>\"\n",
    "url=\"<urlの文字列>\"\n",
    "```\n",
    "例：\n",
    "```py\n",
    "apikey=\"u3RTGnji90okmbhuYTGVr-ZAqwSCXCDErfvbgt\"\n",
    "url=\"https://api.jp-tok.natural-language-understanding.watson.cloud.ibm.com/instances/123456njuhfgNHY-1q2w3e4r5t6yNHY\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey=\"<apileyの文字列>\"\n",
    "url=\"<urlの文字列>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Natural Language Understanding: NaturalLanguageUnderstandingV1インスタンス作成\n",
    "\n",
    "参考: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/watson-nlp.html?context=cpdaas&audience=wdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "def get_NLU(apikey, url):\n",
    "    authenticator = IAMAuthenticator(apikey)\n",
    "    nlu = NaturalLanguageUnderstandingV1(\n",
    "        version='2022-04-07',\n",
    "        authenticator=authenticator\n",
    "    )\n",
    "\n",
    "    nlu.set_service_url(url)\n",
    "    return nlu\n",
    "\n",
    "nlu=get_NLU(apikey, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Natural Language UnderstandingでrefineText列(DATA列を整形した列)の分析を実施\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "from ibm_watson.natural_language_understanding_v1  import Features,  KeywordsOptions,  SentimentOptions\n",
    "\n",
    "keyword_num = 10\n",
    "keyword_relevance = 0.3\n",
    "\n",
    "features = Features(keywords=KeywordsOptions(limit=3), sentiment=SentimentOptions())\n",
    "df_keywords = pd.DataFrame(columns=['keyword1', 'keyword2', 'keyword3', 'keyword4', 'keyword5', 'keyword6',  'keyword7', 'keyword8',  'keyword9', 'keyword10'])\n",
    "df_sentiment = pd.DataFrame(columns=['sentiment_label', 'sentiment_score'])\n",
    "df_keywords_wordcloud  = pd.DataFrame(columns=['keyword', 'sentiment_label', 'sentiment_score'])\n",
    "\n",
    "\n",
    "for text, i in zip(df_poll['refineText'], df_poll.index):\n",
    "    if i % 10 == 0:\n",
    "        print('{0}: Now processing line {1}/{2}.'.format(str(datetime.datetime.now()), i, len(df_poll)))\n",
    " \n",
    "    if not text:\n",
    "        data_row = ['', '', '']\n",
    "        df_keywords.loc[i] = data_row\n",
    "        data_row = ['', np.nan]\n",
    "        df_sentiment.loc[i] = data_row\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        enriched_json = nlu.analyze(text=text, features=features, language='ja').get_result()  # NLU　呼び出し\n",
    "    except ApiException as ex:\n",
    "        print('{}:text: {} -- Method failed with status code {}: {}'.format(i, text, str(ex.code), ex.message))\n",
    "        data_row = ['ERR', str(ex.code), ex.message]\n",
    "        df_keywords.loc[i] = data_row\n",
    "        data_row = ['ERR', np.nan]\n",
    "        df_sentiment.loc[i] = data_row\n",
    "        continue\n",
    "\n",
    "    # Iterate and get KEYWORDS with a confidence of over keyword_relevance\n",
    "    if 'keywords' in enriched_json:\n",
    "        keywords = []\n",
    "        keynum = keyword_num\n",
    "        for kw in enriched_json[\"keywords\"]:\n",
    "            # print(kw[\"text\"])\n",
    "            keynum -= 1\n",
    "            if (float(kw[\"relevance\"]) >= keyword_relevance):\n",
    "                keywords.append(kw[\"text\"])\n",
    "            else:\n",
    "                keywords.append('')\n",
    "        for key_i in range(keynum):\n",
    "            keywords.append('')\n",
    "        df_keywords.loc[i] = keywords\n",
    "    else:\n",
    "        data_row = ['', '', '','', '', '','', '', '','']\n",
    "        df_keywords.loc[i] = data_row\n",
    "\n",
    "    if 'sentiment' in enriched_json:\n",
    "        sentiment = []\n",
    "        sentiment.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n",
    "        sentiment.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n",
    "        df_sentiment.loc[i] = sentiment\n",
    "    else:\n",
    "        data_row = ['N/A', np.nan]\n",
    "        df_sentiment.loc[i] = data_row\n",
    "\n",
    "df_enriched_poll = pd.concat([df_poll, df_sentiment, df_keywords, ], axis=1)\n",
    "print(\"Finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. WatsonStudioのプロジェクトのデータ資産に保存\n",
    "\n",
    "#### **`IBM Cloud Watson Studioのみ`**\n",
    "中身の確認はプロジェクトのデータ資産から！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_file_name = \"analysis.csv\"\n",
    "project.save_data(saved_file_name,df_enriched_poll.to_csv(index=False),overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`IBM Cloud Watson Studio以外`**\n",
    "\n",
    "Local Diskに保存します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_file_name = \"analysis.csv\"\n",
    "# # ローカルに保存 \n",
    "# df_enriched_poll.to_csv(saved_file_name, index=False)\n",
    "# # 中身の確認\n",
    "# !head {saved_file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Word Cloud分析\n",
    "\n",
    "### 8.1 wordcloudの準備\n",
    "\n",
    "日本語フォントは　https://moji.or.jp/ipafont/ipaex00401/　よりダウンロードします。\n",
    "https://moji.or.jp/ipafont/ipaex00401/　のIPAフォントライセンスV1.0を読んで内容に同意の上、実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fontの準備\n",
    "!wget https://moji.or.jp/wp-content/ipafont/IPAexfont/ipaexg00401.zip\n",
    "!unzip ipaexg00401.zip\n",
    "!ls -la  ipaexg00401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud ライブラリの導入\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloudの初期化\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#font_pathは環境で変更してください\n",
    "wordcloud = WordCloud(font_path='ipaexg00401/ipaexg.ttf',\n",
    "                     width=1600,  height=800,  background_color=\"white\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 KeywordでWordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "words=''\n",
    "for key_i in range(keyword_num):\n",
    "    colname='keyword{0}'.format(key_i+1)\n",
    "    tmp_words = ' '.join(df_enriched_poll[colname])\n",
    "    words = words + tmp_words\n",
    "    \n",
    "wordcloud.generate(words)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 センチメント数のグラフ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_enriched_poll['sentiment_label'].value_counts()\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Positive KeywordでWordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_enriched_poll.query('sentiment_label == \"positive\"')\n",
    "\n",
    "words=''\n",
    "for key_i in range(keyword_num):\n",
    "    colname='keyword{0}'.format(key_i+1)\n",
    "    tmp_words = ' '.join(df[colname])\n",
    "    words = words + tmp_words\n",
    "        \n",
    "wordcloud.generate(words)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Negaitive KeywordでWordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_enriched_poll.query('sentiment_label == \"negative\"')\n",
    "\n",
    "words=''\n",
    "for key_i in range(keyword_num):\n",
    "    colname='keyword{0}'.format(key_i+1)\n",
    "    tmp_words = ' '.join(df[colname])\n",
    "    words = words + tmp_words\n",
    "    \n",
    "wordcloud.generate(words)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 positiveコメントベスト20\n",
    "あくまでもコメントのpositiveスコアが高いだけで、主流のコメントではないことにご注意ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_enriched_poll.query('sentiment_label == \"positive\"')\n",
    "df.sort_values('sentiment_score', ascending=False).loc[:,['refineText','sentiment_score']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  8.7 negativeコメント ワースト10\n",
    "あくまでもコメントのnegativeスコアが高い(というかsentiment_scoreが低い)だけで、主流のコメントではないことにご注意ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_enriched_poll.query('sentiment_label == \"negative\"')\n",
    "df.sort_values('sentiment_score').loc[:,['refineText','sentiment_score']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上です。\n",
    "保存したセンチメント情報とキーワード情報のついたファイルもながめてみてください。\n",
    "ファイル名は`analysis.csv`です。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
